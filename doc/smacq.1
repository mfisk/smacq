.\" This -*- nroff -*- file has been generated from
.\" DocBook SGML with docbook-to-man on Debian GNU/Linux.
...\"
...\"	transcript compatibility for postscript use.
...\"
...\"	synopsis:  .P! <file.ps>
...\"
.de P!
\\&.
.fl			\" force out current output buffer
\\!%PB
\\!/showpage{}def
...\" the following is from Ken Flowers -- it prevents dictionary overflows
\\!/tempdict 200 dict def tempdict begin
.fl			\" prolog
.sy cat \\$1\" bring in postscript file
...\" the following line matches the tempdict above
\\!end % tempdict %
\\!PE
\\!.
.sp \\$2u	\" move below the image
..
.de pF
.ie     \\*(f1 .ds f1 \\n(.f
.el .ie \\*(f2 .ds f2 \\n(.f
.el .ie \\*(f3 .ds f3 \\n(.f
.el .ie \\*(f4 .ds f4 \\n(.f
.el .tm ? font overflow
.ft \\$1
..
.de fP
.ie     !\\*(f4 \{\
.	ft \\*(f4
.	ds f4\"
'	br \}
.el .ie !\\*(f3 \{\
.	ft \\*(f3
.	ds f3\"
'	br \}
.el .ie !\\*(f2 \{\
.	ft \\*(f2
.	ds f2\"
'	br \}
.el .ie !\\*(f1 \{\
.	ft \\*(f1
.	ds f1\"
'	br \}
.el .tm ? font underflow
..
.ds f1\"
.ds f2\"
.ds f3\"
.ds f4\"
'\" t 
.ta 8n 16n 24n 32n 40n 48n 56n 64n 72n  
.TH "smacq" "1" 
.SH "NAME" 
smacq \(em System for Modular Analysis and Continuous Queries 
.SH "DESCRIPTION" 
.PP 
The System for Modular Analysis and Continuous Queries 
(SMACQ) is an extensible component system for analyzing streams 
of structured data.  This manpage describes the modules 
included with the system by default.  
 
.PP 
These modules are used by SMACQ's command-line utilities 
\fBsmacqp(1)\fP  and \fBsmacqp(1)\fP       as well as a C API described in \fBsmacq-embed(3)\fP. 
 
.SH "STANDARD INPUT/OUTPUT MODULES" 
.SS "Print" 
.PP 
\fBprint\fP [-v]  [-B]  [-d \fIdelimiter\fP] fields \&...  
.PP 
Print the specified fields for every record that has them If 
the \fB-v\fP option is given, then warnings are 
printed when fields aren't present, and field values are 
preceded by the field name.  If the \fB-B\fP option 
is specified, then output is not buffered (output is flushed 
after each record).  Fields are separated by a delimiter 
string which can be specified by \fB-d\fP and 
defaults to TAB. 
 
.SS "Tabular Input" 
.PP 
\fBtabularinput\fP [-d \fIdelimiter\fP]  [-f \fIfilename\fP]  [field [:type]  \&...]  
.PP 
Read records from STDIN by default, or a filename specified with 
the \fB-f\fP option, one per line, with fields delimited 
by TAB or an alterate delimiter specified by 
\fB-d\fP.  If field names are specified, data 
columns are assigned those names sequentially.  If no field 
names are specified, or there are more columns that field 
names, unnamed fields are named numericaly starting at 1.  
 
.PP 
Field types can be specified by appending a colon and the type name to the end of the field name. 
If no type is sepecified for a field, it is treated as a double if possible, or a string otherwise. 
 
.SS "Packet Capture" 
.PP 
\fBpcaplive\fP [-i \fIinterface\fP]  [-s \fIsnaplen\fP]  [-p]  [\fIfilter\fP \&...]  
.PP 
The \fBpcaplive\fP module reads packets from a 
network interfaces using libpcap.  It can only be used at the 
beginning of a pipeline.  Root privileges are typically 
required to run this module. 
 
.PP 
The \fB-i\fP option specifies an interface to 
listen on (default is \fBany\fP).  The 
\fB-s\fP option specifies the maximum number of 
bytes per packet to capture (default is 68).  The 
\fB-p\fP specifies that the interface should NOT be 
placed in promiscuous mode. 
 
.PP 
An optional filter string is a BPF filter string (see 
\fBtcpdump\fP(1)). 
 
.SS "Packet Trace File" 
.PP 
\fBpcapfile\fP \fIfilename\fP \&...  
.PP 
\fBpcapfile\fP -w \fIfilename\fP  [-l ]  [-s \fImegabytes\fP]  
.PP 
The \fBpcapfile\fP module either reads from or 
writes to a tcpdump-style, libpcap packet trace file.  If the 
module is at the beginning of a pipeline, it reads from a 
file.  Otherwise it writes data to a file. 
 
.PP 
When reading, one or more files must be specified.  Use 
\fB-\fP for stdin.  Input files that are 
compressed with \fBgzip\fP are supported 
automatically.  If the \fB-l\fP option 
is specified then files are read from STDIN instead of  
the arguments. 
 
.PP 
When writing, a single output file must be specified with 
\fB-w\fP. 
 
.PP 
The \fB-s\fP option specifies the maximum file size (in 
megabytes) for output files.  If specified, the output file 
will have a two-digit suffix number appended and output will 
be split between as many files as necessary. 
 
.SS "Socket" 
.PP 
\fBsocket\fP [-p \fIport\fP]  [-h \fIhost\fP]  [-d]  
.PP 
The \fBsocket\fP module is used to send records 
across the network to another instantiation of the 
\fBsocket\fP module.  It can be used in two 
different ways: as a producer who receives data from the 
network, or as a consumer that writes data to a network.  If 
the module is at the beginning of a pipeline, it is assumed to 
be a server.  Otherwise it is a consumer that writes data to 
the network. 
 
.PP 
The \fB-h\fP and \fB-p\fP options 
specify a host and port, respectively.  The host option is required 
for a consumer.  The default port is 3000. 
 
.PP 
The \fB-d\fP option is only valid in the server context. 
If specified, the module will continue to accept new 
connections forever and will never exit.  Without this option, 
the server will accept a single connection, process it until 
it closes, and then terminate. 
 
.SH "FILTER MODULES (BOOLEAN FUNCTIONS)" 
.SS "IP Address Mask" 
.PP 
\fBmask\fP\fIfield\fP [!]\fIaddr/cidr\fP  
.PP 
The "addr/cidr" argument is a CIDR netmask.  If the mask size 
is not specified, 32 is assumed.  An object is filtered out if 
and only if the specified field does not exist or does not 
match the given netmask.  If the address begins with a '!', 
then the logic is reversed and the object is filtered out if 
the field does match the netmask. 
 
.SS "Substring" 
.PP 
\fBsubstr\fP [\fIfield\fP] \fIstring\fP  [ ; \fIstring\fP \&...]  
.PP 
Search for each byte string in the specified field, or in the 
whole data object if no field is given.  If multiple strings 
are given, then each string corresponds to an output channel, 
and the object will be output only on the channel(s) that 
match. 
 
.SS "Last" 
.PP 
\fBlast\fP [-t \fItime\fP]  [\fIfields\fP \&...]  
.PP 
If any fields are specified, treat those fields as a tuple and keep track of the last object seen  
with that tuple value.  After there is no more data, output the objet for each tuple value. 
 
.PP 
The \fB-t\fP option specifies, as a real number, 
the number of seconds between periodic updates.  After the specified 
amount of time, the last object seen for each tuple value will be emitted  
(just as is done at the end of the data stream).  At the end of the update, an 
object of type "\fBrefresh\fP" will be sent with a  
"\fBtimeseries\fP" field of type "\fBtimeval\fP" 
containing the time. 
Note: Time is not the wall-clock time, but is instead the 
time stored in the record in the 
"\fBtimeseries\fP" field of type 
"\fBtimeval\fP".  The \fB-t\fP cannot 
be used with records that do not have this field. 
 
.SS "Filter" 
.PP 
\fBfilter\fP \fIfield [[<=>] value] ...\fP \&...  
.PP 
Filter out all objects in the stream that do not satisfy all 
of the specified criteria.  Expressions can be arbitrarily complex and include AND and OR  
statements and parentheses for grouping. 
 
.PP 
This is the select (sigma) 
operation from relational algebra ("where" in SQL). 
 
.SS "Unique Filter" 
.PP 
\fBuniq\fP [-m \fImegabytes\fP]  \fIfields\fP \&...  
.PP 
Treat the specified field(s) as a tuple and filter out all 
occurrences of duplicate values of that tuple. 
 
.PP 
The \fB-m\fP option specifies that a probabilistic 
algorithm using a fixed amount of memory (specified in 
megabytes) should be employed.  Some records may be mistakenly 
filtered, but some large datasets cannot be processed with a 
perfect algorithm. 
 
.SS "Top" 
.PP 
\fBtop\fP [-m \fImegabytes\fP]  [-r \fIdeviation\fP]  \fIfields\fP \&...  
.PP 
Treat the specified field(s) as a tuple and count the number 
of occurrences of each values of that tuple.  Filter out all 
records except those whose occurrence deviates from the 
average by more than a factor of 
\fBdeviation\fR.  If no 
\fB-r\fP option is specified, the default 
deviation threshold is 1. 
 
.PP 
If \fB-m\fP is specified, then probabilistic 
counters are used, consuming a max of 
\fBmegabytes\fR memory, at the expense of 
some records not being filtered even though they're value is 
rare. 
 
.PP 
It is often useful to follow this module with 
\fBuniq\fP in order to get exact counts for all 
records that pass this filter. 
 
.SS "Head" 
.PP 
\fBhead\fP \fInumber\fP  
.PP 
Pass the first \fBnumber\fR records through 
and then end the pipeline.  Those records will be processed by 
all subsequent modules in the pipeline and the program will 
then terminate. 
 
.SH "ANALYSIS MODULES" 
.SS "Constant Annotation" 
.PP 
\fBconst\fP \fIstring\fP  [\fIfield\fP]  
.PP 
Annotate each object with a field containing the specified string constant. 
If a field name is specified, it will be used.  Otherwise, the name will be the same 
as the value string. 
 
.SS "Counter" 
.PP 
\fBcounter\fP [-f \fIcountname\fP]  [\fIfields\fP \&...]  
.PP 
If no fields are specified, simply count the number of records 
seen.  If one or more fields are specified, treat those fields 
as a tuple and count the number of occurrences of each value 
for that tuple.   
 
.PP 
The count value is added to the record as an annotation of type int and name "counter" 
unless the "-f" is used to specify an alternate name for the field. 
 
.SS "Discrete Probability Density Function" 
.PP 
\fBpdf\fP 
.PP 
Assemble a stream of input records with "count" fields. 
When a "refresh" record is received or the data flow ends, then  
use the "count" fields to calculate the fraction of the total  
that each record is responsible.  Attach this value as a "probability" 
field of type "double". 
calculate then use the  
 
.SS "Project" 
.PP 
\fBproject\fP \fIfields\fP \&...  
.PP 
Replace all objects in the input stream with new objects 
containing only the specified fields.  This is the project (Pi) 
operation from relational algebra ("select <fields>" in SQL). 
 
.SS "Rename" 
.PP 
\fBrename\fP \fIoldfield newfield\fP \&...  
.PP 
Given a list of alternating old and new field names, make a copy of the old field with the new name. 
Combined with the Project module, this can implement the rename (rho) 
operation from relational algebra ("as" in SQL). 
 
.SS "Delta" 
.PP 
\fBdelta\fP \fIxfield\fP  
.PP 
For each data object seen, compute the delta from the previous x field to this current xfield. 
The data object is annotate with a "\fBdelta\fP" 
field of type "\fBdouble\fP" containing the result.  The x field must be convertable to doubles as well. 
 
.SS "Derivative" 
.PP 
\fBderivative\fP \fIyfield\fP \fIxfield\fP  
.PP 
For each data object seen, compute the derivative of the y field with respect to the x field 
between this point and the last object seen.  The data object is annotate with a "\fBderivative\fP" 
field of type "\fBdouble\fP" containing the result.  The x and y fields must be convertable 
to doubles as well. 
 
.SS "Entropy" 
.PP 
\fBentropy\fP 
.PP 
This module expects a series of data objects with "\fBprobability\fP" fields 
and computes the Shannon entropy for that series.   
When the data stream ends or a "\fBrefresh\fP" object is seen, it is assumed 
that every ocurring value has been seen and the entropy for the series is calculated 
and added as an annotation of type \fBdouble\fP to a refresh object.   
See the "\fBlast\fP" module for more information on \fBrefresh\fP objects. 
 
.SS "Flow ID" 
.PP 
\fBflowid\fP [-t \fItime\fP]  [-r ]  \fIfields\fP \&...  
.PP 
Treat the specified field(s) as a tuple and assign a unique 
flow id number to each object based on the typle value.  The 
annotated field is called "flowid".  All but the first packet 
will be filtered out. 
 
.PP 
The \fB-r\fP option specifies that the same flow id 
should be assigned to packets in the reverse direction. 
Separate flow statistics will be kept for each direction. 
 
.PP 
The \fB-t\fP option specifies a number of seconds 
idle time before a flow is timed out.  When it times out a 
REFRESH record with the flows identifying fields (as specified 
in the arguments), the current time (timeseries) and the 
packet and byte counters ("packets", "packetsback", "bytes", 
"bytesout") and the "start" and "finish" times. 
 
.SS "Group-By" 
.PP 
\fBgroupby\fP \fIfields\fP \&...  --   \fIquery\fP \&...  
.PP 
Treat the specified field(s) as a tuple and instantiate the 
specified query for each tuple.  If a record of type "refresh" 
is received, then the pipeline for that tuple will be gracefully 
terminated. 
 
.SS "Time Sort" 
.PP 
\fBfifodelay\fP [-t \fItime\fP]  [-i \fIinput-time-field\fP]  [-o \fIoutput-time-field\fP]  
.PP 
Sort a series of input records and output them sorted by an 
output time field that is specified with the 
\fB-o\fP option and defaults to "timeseries".  All records 
that are past the edge time are immediately updated.  The edge 
time is determined by the input time field (specified with the 
\fB-i\fP option and defaullting to "timseries") and 
a time delay which is specified with the \fB-t\fP 	option which defaults to 0 seconds. 
 
.SH "SEE ALSO" 
.PP 
\fBsmacqq\fP(1),  
\fBsmacqp\fP(1),  
\fBdts\fP(3),  
\fBdts-modules\fP(3),  
\fBsmacq-modules\fP(3), 
\fBsmacq-embed\fP(3) 
 
...\" created by instant / docbook-to-man, Wed 19 Mar 2003, 21:14 
